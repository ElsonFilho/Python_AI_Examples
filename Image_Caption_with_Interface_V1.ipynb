{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nar5DGc2B0oT"
   },
   "source": [
    "**Image Captioning**\n",
    "\n",
    "Another image captioning, now using Gradio interface.\n",
    "The BLIP (Bootstrapped Language Image Pretraining) model can generate captions for images. And a Gradio interface can be used for the BLIP model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZfubHWqWBmu2",
    "outputId": "736db28e-328b-4cd5-9224-f6a9ceda663b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔️ \n"
     ]
    }
   ],
   "source": [
    "# Install gradio and transformers silently (no output shown)\n",
    "\n",
    "import subprocess\n",
    "\n",
    "# Run pip install and capture the output\n",
    "result = subprocess.run(\n",
    "    [\"pip\", \"install\", \"-q\", \"gradio\", \"transformers\"],\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.PIPE,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "# Check if it succeeded and print the last line of stdout\n",
    "if result.returncode == 0:\n",
    "    print(\"✔️\", result.stdout.strip().split('\\n')[-1])  # Only print last line\n",
    "else:\n",
    "    print(\"❌ Installation failed:\")\n",
    "    print(result.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "xpHW3ZavDLZ0"
   },
   "outputs": [],
   "source": [
    "# Import core libraries for image processing and captioning\n",
    "\n",
    "import gradio as gr\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 417,
     "referenced_widgets": [
      "ebcc689b91374e1a8f22716fe2e12437",
      "509c72b03b064606b36d6b3fe299fe53",
      "493e2070c3b64fdf9b3831faeb64e79b",
      "13c8bdfb42be47eaa1c4dfb924b8acde",
      "43589ce55ec3484ba68e797e80e06034",
      "66faa699633d4bcaaf16ec2d8cd2c352",
      "1f1989aeeb69447aa4da8c9fbd0140ae",
      "082fecb6d43245a881d7a921b7b74aa6",
      "9768f999f49c40bfb2c649532766a4c1",
      "9fcbb98a174440b1ab19cac7f5f0c10b",
      "2fd431b94d4448918a38894c53187c84",
      "0b49fd1609e84befb91159dd5c75ab5b",
      "cc657d7ace2746058260b3905ca050bb",
      "a8f16e3a3386486fb323489fcc6f1799",
      "dee737cf18304061966cd2b59db170de",
      "6cc660f521a14909b2ac97778c5190ab",
      "0f6a6de0c87d4c9c94291ad3a1acfbf8",
      "52e68ac323ed4b78888fbbaddcc7816f",
      "5503900cd9d049259e22f48bdf5e8665",
      "37f3944961fb46f3a69582a6b8a73f16",
      "362dfdc433cc4507a24eb65e586d80e6",
      "f8c8799649d84f0f9c6d70764897eef2",
      "d2307da38da84429bb49ffb7204cca0f",
      "2376f18d5e75464a974df635e5f6b366",
      "4853902de4ed42198ae3d0dc411eba09",
      "24bc37c8b44d45558a614de746b3cc59",
      "c7b35798ece84c76a6b776d85f961b57",
      "56925346e63645e3bcd0a7461f5d6725",
      "c303ace3999e4e318bd289f9df6b0afa",
      "b2d578d516254fb0a1fe2a61777b4b89",
      "e0c0380c7ba14e09862b88e825d4f118",
      "4ac999a4cb1d47ef9dd66e72f1013dd3",
      "ab84107c220e42b49f38b4c3b3d821b8",
      "bf9715af028a41e3aac44271afde50b9",
      "4b04ea4eedeb475693a640f9dde8deed",
      "5787beeab63347cbbd91cbd2b3cc9d51",
      "a081aaaa92fa43d99b43485570488cf8",
      "ecd5755ad9544ee785c65e97d2950994",
      "4f9c880fc8e748bf9b9f67037cc79a43",
      "7a5839d6f95a4dc488ac5c6dac5cdd2b",
      "912c08ff02b34e85b2f3a9b70b690bf6",
      "739ea7b3d20e4a888a01ab6a80d50359",
      "a0db7ab75b30441a87edae794adb66ef",
      "68f109dcf7554a9eba309229f250fd60",
      "f79b117c8784495299d11f8753effbf4",
      "27030af07a6a46099392da81725cad4f",
      "829919ee10d04dd287db4c6b9af88d64",
      "3210fce60f4c4a9f8a8bb2f12a46dd63",
      "07c3909130a240f9b39ef461b54e67e7",
      "22d483d8aa7e4a32b2dadc58635821a0",
      "ebd339de03854cf991346dd22208f820",
      "176d05fd44a94ab8b35bc0fb6593d50b",
      "a84dd6147a544cbcae619eac139708f7",
      "1256a04bd36e4ea3a1138245d5754e2f",
      "98d2dade74914f16bb048af95db0e82b",
      "7543e82f7a624b27b4de38933ccb3cd2",
      "8fda7a3cd9a049589004b1b0c6164f01",
      "478edb29934a4704b9fd5c2804569136",
      "86213b3f52ca43c4af871958d3b027b5",
      "2e2bcb26e2ce47639d61c1c5e6f06c64",
      "87015ce17bf04178994d056fa299fa49",
      "e14032d81aa045b1a20f7efc7e9204e6",
      "d3aa4bd561eb49fe83b9cb3de3dfdf99",
      "70b2ab6807294450bd3d01cea4969d7f",
      "c143bd5c704843daa8882228c666d7b6",
      "68d53cfa8ae54565865ad9b4844deeea",
      "09b3ff8f4e244e4c8985abf5bc7b5e24",
      "6ee20087d3324affbdaa843d356229ef",
      "7acaea4e0b2b47bd8275c1e89dac8052",
      "8b817c693a8c48fdb2f19802f9ebfb0f",
      "e0fa7279159f41e0b0b7741932cf05dc",
      "e2d5f7cc70b74efa8de93ed6075bc56d",
      "2f63db3f1b4c4a528779c218f7d20a9a",
      "0c12252951a54e919640ed23d16bf98a",
      "0d6a8ce514c9495c91f29c78e3ebc55c",
      "4a89a4a19a0c4d78ac7a3ddd209136fd",
      "8db4a04b2e2a455a85ce195fc3f61c0e",
      "9dedba69eae84946811ec720076f9d12",
      "68a0196d5edb44478de806ecf8741e6e",
      "dcef120ae24c4b96a6898707f9a4fc79",
      "153d866f25f34f2eafe0e1062f1f858f",
      "a285f63ebed94941bbb05c7d7913c471",
      "640c6b7193f944289f6f441447bade55",
      "eeeca9c142b14ba49349bcdeb281b3a6",
      "77e1ebefde814125922f48a54746cbde",
      "522937271eeb49399ff1f19666f0e738",
      "894601493ec84df6903935a94775cc73",
      "0ca33e2b763d48ee8a77fb2240b9edea"
     ]
    },
    "id": "sEDRq5DyDq2T",
    "outputId": "1e7a51cf-7264-4577-dcec-daf1223db4bf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebcc689b91374e1a8f22716fe2e12437",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/287 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b49fd1609e84befb91159dd5c75ab5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/506 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2307da38da84429bb49ffb7204cca0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf9715af028a41e3aac44271afde50b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f79b117c8784495299d11f8753effbf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7543e82f7a624b27b4de38933ccb3cd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/4.56k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09b3ff8f4e244e4c8985abf5bc7b5e24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dedba69eae84946811ec720076f9d12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the BLIP image captioning processor and model from Hugging Face\n",
    "\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "2EoW-zYDD14j"
   },
   "outputs": [],
   "source": [
    "def generate_caption(image):\n",
    "    \"\"\"\n",
    "    Generates a caption for a given PIL image using BLIP.\n",
    "    \"\"\"\n",
    "    # Prepare image for the model\n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "    # Generate caption tokens\n",
    "    outputs = model.generate(**inputs)\n",
    "\n",
    "    # Decode tokens into a human-readable string\n",
    "    caption = processor.decode(outputs[0], skip_special_tokens=True)\n",
    "    return caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "lSe-GVN_D_fH"
   },
   "outputs": [],
   "source": [
    "def caption_image(image):\n",
    "    \"\"\"\n",
    "    Wrapper function for Gradio interface.\n",
    "    Handles exceptions and returns caption or error message.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return generate_caption(image)\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 647
    },
    "id": "ozR9aB9AEDiS",
    "outputId": "207c4fb0-4600-4c62-9674-1efa38e9bdf7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
      "\n",
      "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
      "* Running on public URL: https://f291a0f08149d02177.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://f291a0f08149d02177.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up the Gradio interface for the image captioning model\n",
    "iface = gr.Interface(\n",
    "    fn=caption_image,\n",
    "    inputs=gr.Image(type=\"pil\"),       # Accepts image uploads as PIL Image\n",
    "    outputs=\"text\",                    # Displays the generated caption as text\n",
    "    title=\"Image Captioning with BLIP\",\n",
    "    description=\"Upload an image to generate a caption.\"\n",
    ")\n",
    "\n",
    "# Launch the app locally\n",
    "iface.launch(server_name=\"127.0.0.1\", server_port=7860)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FEno_OHBEXYp"
   },
   "source": [
    "## 🚀 Demo\n",
    "\n",
    "Check out the live app: [Captioning](https://f291a0f08149d02177.gradio.live/)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
