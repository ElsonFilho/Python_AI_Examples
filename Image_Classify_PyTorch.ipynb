{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNXL0udTIt64s/JYJPakewv"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Image Classification in PyTorch**\n",
        "\n",
        "Image classification is a central task in computer vision. Building better classifiers to classify what object is present in a picture is an active area of research, as it has applications stretching from autonomous vehicles to medical imaging.\n",
        "\n",
        "Such models are perfect to use with Gradio's image input component. In this notebook, we have a web demo to classify images using Gradio. We can build the whole web application in Python."
      ],
      "metadata": {
        "id": "Yd-zwt11HKbq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install gradio silently (no output shown)\n",
        "\n",
        "import subprocess\n",
        "\n",
        "# Run pip install and capture the output\n",
        "result = subprocess.run(\n",
        "    [\"pip\", \"install\", \"-q\", \"gradio\"],\n",
        "    stdout=subprocess.PIPE,\n",
        "    stderr=subprocess.PIPE,\n",
        "    text=True\n",
        ")\n",
        "\n",
        "# Check if it succeeded and print the last line of stdout\n",
        "if result.returncode == 0:\n",
        "    print(\"‚úîÔ∏è\", result.stdout.strip().split('\\n')[-1])  # Only print last line\n",
        "else:\n",
        "    print(\"‚ùå Installation failed:\")\n",
        "    print(result.stderr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "frY8RTdRJuHV",
        "outputId": "a569a00a-b1ee-415d-93b4-b296c4e5375e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úîÔ∏è    ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 62.5/62.5 kB 4.6 MB/s eta 0:00:00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Setting up the image classification model**\n",
        "\n",
        "We will need an image classification model. On this Notebook, we will use a pretrained Resnet-18 model, as it is easily downloadable from PyTorch Hub."
      ],
      "metadata": {
        "id": "GwQ0g4mPH-U2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SNqT4HMMGXqd",
        "outputId": "cbab525e-3f52-4c14-baca-fc0c8fc4f2d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.6.0\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "model = torch.hub.load('pytorch/vision:v0.6.0', 'resnet18', pretrained=True).eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Defining a predict function**\n",
        "\n",
        "Next, we will need to define a function that takes in the user input, which in this case is an image, and returns the prediction. The prediction should be returned as a dictionary whose keys are class name and values are confidence probabilities. We will load the class names from this text file."
      ],
      "metadata": {
        "id": "lo8haDaoIqaS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "\n",
        "# Download human-readable labels for ImageNet.\n",
        "response = requests.get(\"https://git.io/JJkYN\")\n",
        "labels = response.text.split(\"\\n\")\n",
        "\n",
        "def predict(inp):\n",
        " inp = transforms.ToTensor()(inp).unsqueeze(0)\n",
        " with torch.no_grad():\n",
        "  prediction = torch.nn.functional.softmax(model(inp)[0], dim=0)\n",
        "  confidences = {labels[i]: float(prediction[i]) for i in range(1000)}\n",
        " return confidences"
      ],
      "metadata": {
        "id": "maZ1k_WgHspu"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating a Gradio interface**\n",
        "\n",
        "Now that we have our predictive function set up, we can create a Gradio Interface around it.\n",
        "\n",
        "In this case, the input component is a drag-and-drop image component. To create this input, we use Image(type=‚Äúpil‚Äù) which creates the component and handles the preprocessing to convert that to a PIL image.\n",
        "\n",
        "The output component will be a Label, which displays the top labels in a nice form. Since we don't want to show all 1,000 class labels, we will customize it to show only the top 3 images by constructing it as Label(num_top_classes=3).\n",
        "\n",
        "Finally, we'll add one more parameter, the examples, which allows us to prepopulate our interfaces with a few predefined examples."
      ],
      "metadata": {
        "id": "0lPD-WSuJLpv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "gr.Interface(fn=predict,\n",
        "       inputs=gr.Image(type=\"pil\"),\n",
        "       outputs=gr.Label(num_top_classes=3),\n",
        "       examples=[\"/content/lion.jpg\", \"/content/cheetah.jpg\"]).launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 647
        },
        "id": "hHdqehKYJLHJ",
        "outputId": "fe8fe90c-c2ad-4f44-8d4c-6723df53fc3c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://38cbf3704cc4518894.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://38cbf3704cc4518894.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üöÄ Demo\n",
        "\n",
        "Check out the live app: [Image Classification](https://38cbf3704cc4518894.gradio.live/)"
      ],
      "metadata": {
        "id": "XlRMgRRwLNyZ"
      }
    }
  ]
}